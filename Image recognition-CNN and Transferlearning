import zipfile, os
import shutil
import math
import numpy as np
import matplotlib.pyplot as plt

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import load_model
from tensorflow.keras.models import Model
import tensorflow.keras.utils 

#Getting the Data
# downloading the files from dropbox
!wget --no-check-certificate  https://uofi.box.com/shared/static/5tp2foxit52byt2zh9eqnnd8oi3ac9ym.zip -O intel-images.zip

#extrac the zip file downloaded
zip_ref = zipfile.ZipFile('intel-images.zip', 'r')
zip_ref.extractall()

def count_images(dir):
    count = {"buildings": 0, "forest": 0, "glacier": 0,
             "mountain": 0, "sea": 0, "street": 0}
    for i in os.listdir(dir):
        count[i] = len(os.listdir(dir + "/" + i))
    return count

test_count = count_images("intel/seg_test/")
train_count = count_images("intel/seg_train/")

print(test_count, train_count)

test_dir="intel/seg_test/"
train_dir = "intel/seg_train/"
val_dir = "intel/seg_val/"

os.mkdir(val_dir + "/")
for i in os.listdir(train_dir):
  os.mkdir(val_dir + "/" + i)
  for j in os.listdir(train_dir + "/" + i)[:int(len(os.listdir(train_dir + "/" + i)) * 0.2)]:
    os.rename(train_dir + "/" + i + "/" + j, val_dir + "/" + i + "/" + j)
    
val_count=count_images("intel/seg_val")
print(val_count)


# Creating training, test and validation datasets
from tensorflow.keras.preprocessing import image_dataset_from_directory
train_dataset = image_dataset_from_directory(
train_dir,
image_size=(150, 150),
batch_size=32)

validation_dataset = image_dataset_from_directory(
val_dir,
image_size=(150, 150),
batch_size=32)


test_dataset = image_dataset_from_directory(
test_dir,
image_size=(150, 150),
batch_size=32)

1. Create a baseline model

def build_baseline(input_shape, filters):

  #Configuring the model architecture

  #input layer for getting the input image
  input = keras.Input(shape=input_shape)

  #rescaling layer for rescalign pixels to [0,1] range
  x = layers.experimental.preprocessing.Rescaling(1./255)(input)


  for filter in filters:
    #A block of one conv+batchnorm+relu  layers for extractign features
    x =layers.Conv2D(filters=filter, kernel_size=3, padding="same", use_bias=False)(x)
    x= layers.BatchNormalization()(x)
    x= layers.ReLU()(x)

    #max pooling for downsampling
    x = layers.MaxPooling2D(pool_size=2, padding="same")(x)

  #Global Average pooling. This will get an input of shape (height, width, channels) the average of each feature map and returns a vector of size channels.
  x = layers.GlobalAveragePooling2D()(x)

  #The final output layer has 6 neurons with softmax activation to output the probability of the target class 
  output=layers.Dense(6, activation="softmax")(x)
  #create a model and set its input and output and return it
  model = keras.Model(inputs=input, outputs=output)
  return model
baseline=build_baseline(input_shape=(150,150,3), filters=[32,64,128, 256])
print(baseline.summary())

lr_schedule = keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate=1e-3,
    decay_steps=1000,
    decay_rate=0.9)

#compiling the model
opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule)
baseline.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'], optimizer=opt) 

#callback for early stopping. stop the training if the validation_loss does not improve after 10 epochs
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, min_delta=1e-4, restore_best_weights=True)

#save the checkpointed model in your google drive cnn_lab directory: "drive/MyDrive/cnn_lab/baseline_checkpoint" . 
checkpoint= keras.callbacks.ModelCheckpoint( filepath="drive/MyDrive/cnn_lab/baseline_checkpoint",save_best_only=True, monitor="val_loss")

history = baseline.fit(
            train_dataset,
            validation_data = validation_dataset,
            epochs = 50,
            verbose = 1,
            callbacks=[early_stopping, checkpoint])
            

#getting train and validation accuracies
train_acc_CNN = history.history['accuracy']
val_acc_CNN = history.history['val_accuracy']

#getting train and validation losses
train_loss_CNN = history.history['loss']
val_loss_CNN = history.history['val_loss']
epochs = range(1, len(train_loss_CNN) + 1)

#plotting the training and validation accurracies
plt.plot(epochs, train_acc_CNN, 'b', label='Training acc')
plt.plot(epochs, val_acc_CNN, 'r', label='Validation acc')
plt.title('Training and validation accuracy for CNN')
plt.legend()
plt.figure()

#plotting the train and validaiton losses
plt.plot(epochs, train_loss_CNN, 'b', label='Training loss')
plt.plot(epochs, val_loss_CNN, 'r', label='Validation loss')
plt.title('Training and validation loss for CNN')
plt.legend()

plt.show()

2. Using data augmentation

  def data_augmentation(x):
  x= layers.experimental.preprocessing.RandomFlip("horizontal")(x)
  x=layers.experimental.preprocessing.RandomRotation(0.1)(x)
  x=layers.experimental.preprocessing.RandomZoom(0.2)(x)
  return x
  
  def residual_block(x, filter):
  
  residual = x
  
  # x goes through a block consisting of two conv2d+batchnorm+reul as well as a max pooling

  x = layers.SeparableConv2D(filters=filter, kernel_size=3, padding="same", use_bias=False )(x)
  x= layers.BatchNormalization()(x)
  x= layers.ReLU()(x)

  x = layers.SeparableConv2D(filters=filter, kernel_size=3, padding="same", use_bias=False )(x)
  x= layers.BatchNormalization()(x)
  x= layers.ReLU()(x)

  #max pooling for downsampling
  x = layers.MaxPooling2D(pool_size=2, padding="same")(x)
  
  # After going through the above block x now has "filter" channels and its feature map is downsampled to half by max pooling
  #so we need to use a 1*1 convolution with stride=2 to downsample residual and change its numebr of channels to "filter".
  residual = layers.Conv2D(filters=filter, kernel_size=1, strides=2, use_bias=False)(residual)
  
  #Having batchnormalization layer as usual after convolution
  residual= layers.BatchNormalization()(residual)


  x = layers.add([x, residual])
  return x
  
  def build_baseline(input_shape, filters):

  #Configuring the model architecture

  #input layer for getting the input image
  input = keras.Input(shape=input_shape)

  x=data_augmentation(input)

  #rescaling layer for rescalign pixels to [0,1] range
  x = layers.experimental.preprocessing.Rescaling(1./255)(input)


  for filter in filters:
     x= residual_block(x, filter)

  #Global Average pooling. This will get an input of shape (height, width, channels) the average of each feature map and returns a vector of size channels.
  x = layers.GlobalAveragePooling2D()(x)

  x= layers.Dropout(0.5)(x)

  #The final output layer has 6 neurons with softmax activation to output the probability of the target class 
  output=layers.Dense(6, activation="softmax")(x)
  #create a model and set its input and output and return it
  model = keras.Model(inputs=input, outputs=output)
  return model

  baseline_depthwise_aug=build_baseline(input_shape=(150,150,3), filters=[32,64,128,256,512])
  print(baseline_depthwise_aug.summary())
  
  lr_schedule = keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate=1e-3,
    decay_steps=1000,
    decay_rate=0.9)

#compiling the model
opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule)
baseline_depthwise_aug.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'], optimizer=opt) 

#callback for early stopping. stop the training if the validation_loss does not improve after 10 epochs
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, min_delta=1e-4, restore_best_weights=True)

#save the checkpointed model in your google drive cnn_lab directory: "drive/MyDrive/cnn_lab/baseline_checkpoint" . 
checkpoint= keras.callbacks.ModelCheckpoint( filepath="drive/MyDrive/cnn_lab/baseline_checkpoint",save_best_only=True, monitor="val_loss")

history = baseline_depthwise_aug.fit(
            train_dataset,
            validation_data = validation_dataset,
            epochs = 70,
            verbose = 1,
            callbacks=[early_stopping, checkpoint])
            
 3. Applying transfer learning using Densenet121 model

Feature extraction

from tensorflow.keras.applications import DenseNet121
conv_base = DenseNet121(weights='imagenet',
include_top=False,
input_shape=(150, 150, 3))

#freeze the weight of the convolutional base
conv_base.trainable=False

# get the summary of the model to view its architecture
conv_base.summary()

def build_pretrained(input_shape):

  #Configuring the model architecture

  #input layer for getting the input image
  input = keras.Input(shape=input_shape)

  #Add the data_augmentation layers here:
  x=data_augmentation(input)

  #rescaling layer for rescalign pixels to [0,1] range
  x = layers.experimental.preprocessing.Rescaling(1./255)(x)
  
  #Using the pre-trained conv_base
  x = conv_base(x)

  #Global Average pooling. This will get an input of shape (height, width, channels) the average of each feature map and returns a vector of size channels.
  x = layers.GlobalAveragePooling2D()(x)

  #x= layers.Dropout(0.3)(x)

  #The final output layer has 6 neurons with softmax activation to output the probability of the target class 
  output=layers.Dense(6, activation="softmax")(x)
  
  #create a model and set its input and output and return it
  model = keras.Model(inputs=input, outputs=output)
  return model

pretrained_model=build_pretrained(input_shape=(150,150,3))
print(pretrained_model.summary())

lr_schedule = keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate=1e-3,
    decay_steps=1000,
    decay_rate=0.9)

#compiling the mode
opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule)
pretrained_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'], optimizer=opt) 

#callback for early stopping. stop the training if the validation_loss does not improve after 10 epochs
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, min_delta=1e-4, restore_best_weights=True)

history = pretrained_model.fit(
            train_dataset,
            validation_data = validation_dataset,
            epochs = 50,
            verbose = 1,
            callbacks=[early_stopping])
            
 #Fine tuning
 
 conv_base.trainable = True

#set trainable to False for all layers except the last 9 , that is freeze the weights for all layers except the last 9 layers
for layer in conv_base.layers[:-9]:
    layer.trainable=False

#set trainable to True for the convolutional layhers in the last 9 layers ( the last convolutional block in DENSENET121). 
for layer in conv_base.layers[-9:]:
    # we only want to unfreez the convolutional layers (batch normalizataion layers remain frozen)
    if layer.name.endswith("conv"):
      layer.trainable=True

print(pretrained_model.summary())

lr_schedule = keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate=1e-5,
    decay_steps=1000,
    decay_rate=0.9)

#compiling the model
opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule)
pretrained_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'], optimizer=opt) 

history = pretrained_model.fit(
            train_dataset,
            validation_data = validation_dataset,
            epochs = 50,
            verbose = 1,
            callbacks=[early_stopping])
            
4. Evaluating the performance of the model on test data

test_dataset = tf.keras.preprocessing.image_dataset_from_directory(
    test_dir,
    shuffle=False,
    batch_size=32,
    image_size=(150, 150)
)

test_loss, test_acc = pretrained_model.evaluate(test_dataset)
print('test acc:', test_acc)

94/94 [==============================] - 6s 67ms/step - loss: 0.3086 - accuracy: 0.8947
test acc: 0.8946666717529297

predictions = pretrained_model.predict(test_dataset)
predictions = np.argmax(predictions, axis=1)
true_class_indices = np.concatenate([y for x, y in test_dataset], axis=0)

5. Confusion matrix

from sklearn.metrics import confusion_matrix
cf_matrix= confusion_matrix(true_class_indices, predictions)
print(cf_matrix)

[[393   1   2   2   4  35]
 [  0 469   1   2   0   2]
 [  0   3 463  60  26   1]
 [  3   3  75 424  19   1]
 [  4   2  10   4 486   4]
 [ 47   0   3   0   2 449]]
 
 
